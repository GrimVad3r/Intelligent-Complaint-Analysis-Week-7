{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a071818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8cb7b8",
   "metadata": {},
   "source": [
    "# KIAM8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d85a1f",
   "metadata": {},
   "source": [
    "## Task 2: Text Chunking, Embedding, and Vector Store Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deab661",
   "metadata": {},
   "source": [
    "Objective: convert the cleaned text narratives into a format suitable for efficient semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e225777",
   "metadata": {},
   "source": [
    "### Sub TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92afe6a",
   "metadata": {},
   "source": [
    "#### ●\tCreate a stratified sample of 10,000-15,000 complaints from your cleaned dataset:\n",
    "        ○\tEnsure proportional representation across all five product categories.\n",
    "        ○\tDocument your sampling strategy in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0744758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7626e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sample distribution:\n",
      "product_category\n",
      "Credit Card        5021\n",
      "Savings Account    3712\n",
      "Money Transfers    2576\n",
      "Personal Loan       691\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "df_clean = pd.read_csv('../data/processed/filtered_complaints.csv')\n",
    "\n",
    "# Create stratified sample of 12,000 complaints\n",
    "sample_size = 12000\n",
    "\n",
    "# Calculate samples per category (proportional)\n",
    "category_counts = df_clean['product_category'].value_counts()\n",
    "sample_distribution = (category_counts / len(df_clean) * sample_size).round().astype(int)\n",
    "\n",
    "print(\"Target sample distribution:\")\n",
    "print(sample_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d855643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual sample size: 12000\n",
      "\n",
      "Actual distribution:\n",
      "product_category\n",
      "Credit Card        5021\n",
      "Savings Account    3712\n",
      "Money Transfers    2576\n",
      "Personal Loan       691\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample from each category\n",
    "sampled_dfs = []\n",
    "for category, n_samples in sample_distribution.items():\n",
    "    category_df = df_clean[df_clean['product_category'] == category]\n",
    "    if len(category_df) >= n_samples:\n",
    "        sampled = category_df.sample(n=n_samples, random_state=42)\n",
    "    else:\n",
    "        sampled = category_df\n",
    "    sampled_dfs.append(sampled)\n",
    "\n",
    "df_sample = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nActual sample size: {len(df_sample)}\")\n",
    "print(\"\\nActual distribution:\")\n",
    "print(df_sample['product_category'].value_counts())\n",
    "\n",
    "# Save sample\n",
    "df_sample.to_csv('../data/processed/sample_complaints.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48c0ee",
   "metadata": {},
   "source": [
    "#### ●\tLong narratives are often ineffective when embedded as a single vector. Implement a text chunking strategy.\n",
    "        ○\tUse a library like LangChain's RecursiveCharacterTextSplitter or write your own function.\n",
    "        ○\tExperiment with chunk_size and chunk_overlap to find a good balance. Justify your final choice in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be2aff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scripts Path Succcessfully Added to the Environment\n"
     ]
    }
   ],
   "source": [
    "# Import Scripts Path to environment to load predefined scripts\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "cwd = os.getcwd()\n",
    "scripts_path = os.path.join(cwd,'..','src')\n",
    "scripts_abs_path = os.path.abspath(scripts_path)\n",
    "\n",
    "if scripts_path not in sys.path and os.path.isdir(scripts_abs_path):\n",
    "    sys.path.append(scripts_abs_path)\n",
    "    print ('Scripts Path Succcessfully Added to the Environment')\n",
    "else:\n",
    "    print('Invalid Scripts Path or Scripts Path Already Added to the Environemnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57cd4593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:text_transformations:Chunking complete. Created 36098 chunks from 12000 rows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 36098\n",
      "Average chunks per complaint: 3.01\n"
     ]
    }
   ],
   "source": [
    "# Import Helper Scripts\n",
    "\n",
    "from text_transformations import ComplaintChunker\n",
    "\n",
    "# Apply chunking\n",
    "chunker = ComplaintChunker(chunk_size=500, chunk_overlap=50)\n",
    "df_chunks = chunker.chunk_dataset(df_sample)\n",
    "\n",
    "print(f\"Total chunks created: {len(df_chunks)}\")\n",
    "print(f\"Average chunks per complaint: {len(df_chunks) / len(df_sample):.2f}\")\n",
    "\n",
    "# Save chunks\n",
    "df_chunks.to_csv('../data/processed/complaint_chunks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e9cb716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:text_embedding:Loading model: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:text_embedding:Model loaded successfully on cpu\n",
      "Batches: 100%|██████████| 1129/1129 [07:08<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (36098, 384)\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Import Helper Scripts\n",
    "\n",
    "from text_embedding import EmbeddingGenerator\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating embeddings...\")\n",
    "embed_gen = EmbeddingGenerator()\n",
    "embeddings = embed_gen.generate_embeddings(df_chunks['text'].tolist())\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embed_gen.embedding_dim}\")\n",
    "\n",
    "# Add embeddings to dataframe\n",
    "df_chunks['embedding'] = embeddings.tolist()\n",
    "\n",
    "# Save chunks with embeddings\n",
    "df_chunks.to_parquet('../data/processed/chunks_with_embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "218224a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Helper Scripts\n",
    "\n",
    "import importlib\n",
    "import text_vectorization\n",
    "\n",
    "# Force the module to refresh from disk\n",
    "importlib.reload(text_vectorization)\n",
    "\n",
    "# Re-pull the classes into the local namespace\n",
    "from text_vectorization import FAISSVectorStore, ChromaVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "569ac6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:text_vectorization:Added 36098 vectors. Total store size: 36098\n",
      "INFO:text_vectorization:Successfully persisted FAISS index and metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 36098 chunks\n"
     ]
    }
   ],
   "source": [
    "# Using Both FAISS & Chroma DB to Create Verctor Store\n",
    "\n",
    "#1. Using FAISS\n",
    "\n",
    "# Create and populate vector store\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "vector_store_dir = '../vector_store/FAISS/'\n",
    "#vector_store_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Creating FAISS index...\")\n",
    "vector_store = FAISSVectorStore(embedding_dim=384)\n",
    "\n",
    "# Prepare metadata\n",
    "metadata_list = df_chunks.drop('embedding', axis=1).to_dict('records')\n",
    "\n",
    "# Add to vector store\n",
    "vector_store.add_embeddings(\n",
    "    embeddings=np.array(df_chunks['embedding'].tolist()),\n",
    "    metadata=metadata_list\n",
    ")\n",
    "\n",
    "# Save vector store\n",
    "vector_store.save(\n",
    "    index_path=str(vector_store_dir)+'faiss_index.bin',\n",
    "    metadata_path=str(vector_store_dir)+'metadata.pkl'\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {len(metadata_list)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41afc330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:text_vectorization:Successfully initialized ChromaDB collection: complaints\n",
      "INFO:text_vectorization:Starting ingestion of 36098 documents.\n",
      "INFO:text_vectorization:Successfully added batch 1\n",
      "INFO:text_vectorization:Successfully added batch 2\n",
      "INFO:text_vectorization:Successfully added batch 3\n",
      "INFO:text_vectorization:Successfully added batch 4\n",
      "INFO:text_vectorization:Successfully added batch 5\n",
      "INFO:text_vectorization:Successfully added batch 6\n",
      "INFO:text_vectorization:Successfully added batch 7\n",
      "INFO:text_vectorization:Successfully added batch 8\n",
      "INFO:text_vectorization:Successfully added batch 9\n",
      "INFO:text_vectorization:Successfully added batch 10\n",
      "INFO:text_vectorization:Successfully added batch 11\n",
      "INFO:text_vectorization:Successfully added batch 12\n",
      "INFO:text_vectorization:Successfully added batch 13\n",
      "INFO:text_vectorization:Successfully added batch 14\n",
      "INFO:text_vectorization:Successfully added batch 15\n",
      "INFO:text_vectorization:Successfully added batch 16\n",
      "INFO:text_vectorization:Successfully added batch 17\n",
      "INFO:text_vectorization:Successfully added batch 18\n",
      "INFO:text_vectorization:Successfully added batch 19\n",
      "INFO:text_vectorization:Successfully added batch 20\n",
      "INFO:text_vectorization:Successfully added batch 21\n",
      "INFO:text_vectorization:Successfully added batch 22\n",
      "INFO:text_vectorization:Successfully added batch 23\n",
      "INFO:text_vectorization:Successfully added batch 24\n",
      "INFO:text_vectorization:Successfully added batch 25\n",
      "INFO:text_vectorization:Successfully added batch 26\n",
      "INFO:text_vectorization:Successfully added batch 27\n",
      "INFO:text_vectorization:Successfully added batch 28\n",
      "INFO:text_vectorization:Successfully added batch 29\n",
      "INFO:text_vectorization:Successfully added batch 30\n",
      "INFO:text_vectorization:Successfully added batch 31\n",
      "INFO:text_vectorization:Successfully added batch 32\n",
      "INFO:text_vectorization:Successfully added batch 33\n",
      "INFO:text_vectorization:Successfully added batch 34\n",
      "INFO:text_vectorization:Successfully added batch 35\n",
      "INFO:text_vectorization:Successfully added batch 36\n",
      "INFO:text_vectorization:Successfully added batch 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB vector store created successfully\n"
     ]
    }
   ],
   "source": [
    "# 2. Using Chroma DB\n",
    "\n",
    "chroma_store = ChromaVectorStore()\n",
    "chroma_store.add_documents(df_chunks)\n",
    "print(\"ChromaDB vector store created successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
