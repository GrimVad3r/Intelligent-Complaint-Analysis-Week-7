{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a071818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8cb7b8",
   "metadata": {},
   "source": [
    "# KIAM8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d85a1f",
   "metadata": {},
   "source": [
    "## Task 2: Text Chunking, Embedding, and Vector Store Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deab661",
   "metadata": {},
   "source": [
    "Objective: convert the cleaned text narratives into a format suitable for efficient semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e225777",
   "metadata": {},
   "source": [
    "### Sub TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92afe6a",
   "metadata": {},
   "source": [
    "#### ●\tCreate a stratified sample of 10,000-15,000 complaints from your cleaned dataset:\n",
    "        ○\tEnsure proportional representation across all five product categories.\n",
    "        ○\tDocument your sampling strategy in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0744758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7626e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sample distribution:\n",
      "product_category\n",
      "Credit Card        5021\n",
      "Savings Account    3712\n",
      "Money Transfers    2576\n",
      "Personal Loan       691\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "df_clean = pd.read_csv('../data/processed/filtered_complaints.csv')\n",
    "\n",
    "# Create stratified sample of 12,000 complaints\n",
    "sample_size = 12000\n",
    "\n",
    "# Calculate samples per category (proportional)\n",
    "category_counts = df_clean['product_category'].value_counts()\n",
    "sample_distribution = (category_counts / len(df_clean) * sample_size).round().astype(int)\n",
    "\n",
    "print(\"Target sample distribution:\")\n",
    "print(sample_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d855643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual sample size: 12000\n",
      "\n",
      "Actual distribution:\n",
      "product_category\n",
      "Credit Card        5021\n",
      "Savings Account    3712\n",
      "Money Transfers    2576\n",
      "Personal Loan       691\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample from each category\n",
    "sampled_dfs = []\n",
    "for category, n_samples in sample_distribution.items():\n",
    "    category_df = df_clean[df_clean['product_category'] == category]\n",
    "    if len(category_df) >= n_samples:\n",
    "        sampled = category_df.sample(n=n_samples, random_state=42)\n",
    "    else:\n",
    "        sampled = category_df\n",
    "    sampled_dfs.append(sampled)\n",
    "\n",
    "df_sample = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nActual sample size: {len(df_sample)}\")\n",
    "print(\"\\nActual distribution:\")\n",
    "print(df_sample['product_category'].value_counts())\n",
    "\n",
    "# Save sample\n",
    "df_sample.to_csv('../data/processed/sample_complaints.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48c0ee",
   "metadata": {},
   "source": [
    "#### ●\tLong narratives are often ineffective when embedded as a single vector. Implement a text chunking strategy.\n",
    "        ○\tUse a library like LangChain's RecursiveCharacterTextSplitter or write your own function.\n",
    "        ○\tExperiment with chunk_size and chunk_overlap to find a good balance. Justify your final choice in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2aff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scripts Path Succcessfully Added to the Environment\n"
     ]
    }
   ],
   "source": [
    "# Import Scripts Path to environment to load predefined scripts\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "cwd = os.getcwd()\n",
    "scripts_path = os.path.join(cwd,'..','src')\n",
    "scripts_abs_path = os.path.abspath(scripts_path)\n",
    "\n",
    "if scripts_path not in sys.path and os.path.isdir(scripts_abs_path):\n",
    "    sys.path.append(scripts_abs_path)\n",
    "    print ('Scripts Path Succcessfully Added to the Environment')\n",
    "else:\n",
    "    print('Invalid Scripts Path or Scripts Path Already Added to the Environemnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57cd4593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henokt\\OneDrive - Inchcape\\Documents\\GitHub\\Intelligent-Complaint-Analysis-Week-7\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 36227\n",
      "Average chunks per complaint: 3.02\n"
     ]
    }
   ],
   "source": [
    "# Import Helper Scripts\n",
    "\n",
    "from text_transformations import ComplaintChunker\n",
    "\n",
    "# Apply chunking\n",
    "chunker = ComplaintChunker(chunk_size=500, chunk_overlap=50)\n",
    "df_chunks = chunker.chunk_dataset(df_sample)\n",
    "\n",
    "print(f\"Total chunks created: {len(df_chunks)}\")\n",
    "print(f\"Average chunks per complaint: {len(df_chunks) / len(df_sample):.2f}\")\n",
    "\n",
    "# Save chunks\n",
    "df_chunks.to_csv('../data/processed/complaint_chunks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e9cb716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n",
      "Using CPU for embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1133/1133 [03:51<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (36227, 384)\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Import Helper Scripts\n",
    "\n",
    "from text_embedding import EmbeddingGenerator\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating embeddings...\")\n",
    "embed_gen = EmbeddingGenerator()\n",
    "embeddings = embed_gen.generate_embeddings(df_chunks['text'].tolist())\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embed_gen.embedding_dim}\")\n",
    "\n",
    "# Add embeddings to dataframe\n",
    "df_chunks['embedding'] = embeddings.tolist()\n",
    "\n",
    "# Save chunks with embeddings\n",
    "df_chunks.to_parquet('../data/processed/chunks_with_embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218224a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Helper Scripts\n",
    "\n",
    "import importlib\n",
    "import text_vectorization\n",
    "\n",
    "# Force the module to refresh from disk\n",
    "importlib.reload(text_vectorization)\n",
    "\n",
    "# Re-pull the classes into the local namespace\n",
    "from text_vectorization import FAISSVectorStore, ChromaVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "569ac6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index...\n",
      "Vector store created with 36227 chunks\n"
     ]
    }
   ],
   "source": [
    "# Using Both FAISS & Chroma DB to Create Verctor Store\n",
    "\n",
    "#1. Using FAISS\n",
    "\n",
    "# Create and populate vector store\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "vector_store_dir = '../vector_store/FAISS/'\n",
    "#vector_store_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Creating FAISS index...\")\n",
    "vector_store = FAISSVectorStore(embedding_dim=384)\n",
    "\n",
    "# Prepare metadata\n",
    "metadata_list = df_chunks.drop('embedding', axis=1).to_dict('records')\n",
    "\n",
    "# Add to vector store\n",
    "vector_store.add_embeddings(\n",
    "    embeddings=np.array(df_chunks['embedding'].tolist()),\n",
    "    metadata=metadata_list\n",
    ")\n",
    "\n",
    "# Save vector store\n",
    "vector_store.save(\n",
    "    index_path=str(vector_store_dir)+'faiss_index.bin',\n",
    "    metadata_path=str(vector_store_dir)+'metadata.pkl'\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {len(metadata_list)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41afc330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch 1/37\n",
      "Added batch 2/37\n",
      "Added batch 3/37\n",
      "Added batch 4/37\n",
      "Added batch 5/37\n",
      "Added batch 6/37\n",
      "Added batch 7/37\n",
      "Added batch 8/37\n",
      "Added batch 9/37\n",
      "Added batch 10/37\n",
      "Added batch 11/37\n",
      "Added batch 12/37\n",
      "Added batch 13/37\n",
      "Added batch 14/37\n",
      "Added batch 15/37\n",
      "Added batch 16/37\n",
      "Added batch 17/37\n",
      "Added batch 18/37\n",
      "Added batch 19/37\n",
      "Added batch 20/37\n",
      "Added batch 21/37\n",
      "Added batch 22/37\n",
      "Added batch 23/37\n",
      "Added batch 24/37\n",
      "Added batch 25/37\n",
      "Added batch 26/37\n",
      "Added batch 27/37\n",
      "Added batch 28/37\n",
      "Added batch 29/37\n",
      "Added batch 30/37\n",
      "Added batch 31/37\n",
      "Added batch 32/37\n",
      "Added batch 33/37\n",
      "Added batch 34/37\n",
      "Added batch 35/37\n",
      "Added batch 36/37\n",
      "Added batch 37/37\n",
      "ChromaDB vector store created successfully\n"
     ]
    }
   ],
   "source": [
    "# 2. Using Chroma DB\n",
    "\n",
    "chroma_store = ChromaVectorStore()\n",
    "chroma_store.add_documents(df_chunks)\n",
    "print(\"ChromaDB vector store created successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
